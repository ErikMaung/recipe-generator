{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1551b-9bc1-4694-9d9c-7154b1b7414c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I. Import All Packages for Review Generator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661fb9c-46a9-43bf-8144-7e4e63bcd681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# II. Data Upload\n",
    "#     A. If on Colab, navigate to the left sidebar and select these icons: 'Files' > 'Upload' and import `reviews.csv`\n",
    "#     B. If on standalone, ensure `reviews.csv` is in your environment before running the following block:\n",
    "#     C. Possible Error:\n",
    "#            You may receive the following error. `ParserError: Error tokenizing data. C error: EOF inside string starting at row 25310`.\n",
    "#            Simply wait and run this code block again. The row number should continually increase until eventually it successfully\n",
    "#            tokenizes the entire file.\n",
    "data = pd.read_csv(\"reviews.csv\")\n",
    "data.head()\n",
    "\n",
    "# Expected Output:\n",
    "#                                               review\tsentiment\n",
    "# 0\tOne of the other reviewers has mentioned that ...\t  positive\n",
    "# 1\tA wonderful little production. <br /><br />The...\t  positive\n",
    "# 2\tI thought this was a wonderful way to spend ti...\t  positive\n",
    "# 3\tBasically there's a family where a little boy ...\t  negative\n",
    "# 4\tPetter Mattei's \"Love in the Time of Money\" is...\t  positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474fe67-8c45-4a9e-a1ab-7fe35a228463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# III. Data Attributes\n",
    "#     A. Averages\n",
    "def averages(text):\n",
    "    \"\"\"\n",
    "    This function calculates the average number of words per sentence and the average number of sentences per entry.\n",
    "    @param text: pandas series; array of strings\n",
    "    @rvalue:\n",
    "    @rvalue:\n",
    "    \"\"\"\n",
    "    total_sentences = 0\n",
    "    total_words = 0\n",
    "    # for each processed document of the text\n",
    "    for doc in nlp.pipe(text, disable=[\"ner\", \"tagger\"]): # nlp.pipe includes different components of the text\n",
    "        sentences = list(doc.sents) # extract all sentences for each entry\n",
    "        total_sentences += len(sentences) # total number of sentences in text\n",
    "        total_words += sum([len(sentence) for sentence in sentences]) # total number of words in text\n",
    "    \n",
    "    avg_words_per_sentence = total_words / total_sentences if total_sentences else 0 # average number of words per sentence\n",
    "    avg_num_sentences = total_sentences / len(text) if len(text) else 0 # average number of sentences across text\n",
    "    \n",
    "    return avg_words_per_sentence, avg_num_sentences\n",
    "\n",
    "avg_words, avg_sent = averages(data['review'][:len(data['review'])//10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6087343-8cdd-4c82-801b-eda232b8e182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     B. Average Computation\n",
    "avg_words, avg_sent\n",
    "# Expected Output:\n",
    "# (20.16343979755327, 13.3566)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f9df4-4262-4f6e-b45d-7919474dee06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IV. Data Truncation \n",
    "#     C. Truncate text\n",
    "def truncate_text(text, min_words, min_sent):\n",
    "    \"\"\"\n",
    "    This function takes in a text and truncates it so that there are only two sentences and if the number of words in the \n",
    "    text is less than min_words, it adds another sentence to reach at least min_words.\n",
    "    \n",
    "    \"\"\"\n",
    "    doc = nlp(text) # create a document that stores different components of text\n",
    "    sentences = list(doc.sents) # list of sentences of text\n",
    "    \n",
    "    new_text = []\n",
    "    word_count = 0 # records number of words in text\n",
    "    sent_count = 0 # records number of sentences in text\n",
    "    \n",
    "    # for each sentence in text\n",
    "    for sentence in sentences:\n",
    "        sent_word_count = len(sentence.text.split()) # count number of words in each sentence\n",
    "        # add a sentence to meet requirements\n",
    "        if word_count + sent_word_count <= min_words or sent_count < min_sent:\n",
    "            new_text.append(sentence.text) # add sentence\n",
    "            word_count += sent_word_count # keep track of words added\n",
    "            sent_count += 1 # keep track of sentences added\n",
    "        # once requirements are met break\n",
    "        if word_count >= min_words and sent_count >= min_sent:\n",
    "            break\n",
    "    \n",
    "    # if there are less than min_words in text, and no remaining sentences to add, keep text as is\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be2cc9-4444-4fe5-b151-4a661b3efd4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     D. Truncation\n",
    "new_data = data.iloc[:len(data)//10].copy()\n",
    "new_data[\"new_data\"] = new_data[\"review\"].apply(lambda x: truncate_text(x, min_words = 50, min_sent = 2))\n",
    "new_data[:10]\n",
    "# Expected Output:\n",
    "#                                             review\tsentiment\tnew_data\n",
    "# 0\tOne of the other reviewers has mentioned that ...\tpositive\tOne of the other reviewers has mentioned that ...\n",
    "# 1\tA wonderful little production. <br /><br />The...\tpositive\tA wonderful little production. <br /><br />The...\n",
    "# 2\tI thought this was a wonderful way to spend ti...\tpositive\tI thought this was a wonderful way to spend ti...\n",
    "# 3\tBasically there's a family where a little boy ...\tnegative\tBasically there's a family where a little boy ...\n",
    "# 4\tPetter Mattei's \"Love in the Time of Money\" is...\tpositive\tPetter Mattei's \"Love in the Time of Money\" is...\n",
    "# 5\tProbably my all-time favorite movie, a story o...\tpositive\tProbably my all-time favorite movie, a story o...\n",
    "# 6\tI sure would like to see a resurrection of a u...\tpositive\tI sure would like to see a resurrection of a u...\n",
    "# 7\tThis show was an amazing, fresh & innovative i...\tnegative\tThis show was an amazing, fresh & innovative i...\n",
    "# 8\tEncouraged by the positive comments about this...\tnegative\tEncouraged by the positive comments about this...\n",
    "# 9\tIf you like original gut wrenching laughter yo...\tpositive\tIf you like original gut wrenching laughter yo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e03860-4ec5-4d48-b666-6710e8174bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# V. Preprocessing Data\n",
    "def clean(data, column):\n",
    "    \"\"\"\n",
    "    This function takes in a data frame and column name of the text data. It converts all letters to lowercase, removes HTML tags,\n",
    "    removes punctuation, removes unnecessary spaces, and removes duplicates.\n",
    "    @param data: data frame with string column\n",
    "    @param column: column name of string column\n",
    "    \"\"\"\n",
    "    clean_data = (data[column] # Reduce the data to a specific column\n",
    "                .str.lower() # Convert to lowercase\n",
    "                .apply(lambda x: re.sub('<.*?>', ' ', x)) # Replace HTML tags with a space\n",
    "                .apply(lambda x: re.sub(r'[^\\w\\s]', '', x)) # Remove punctuation\n",
    "                .apply(lambda x: re.sub(r'\\s{2,}', ' ', x)) # Replace 2+ consecutive spaces with a single space\n",
    "                .drop_duplicates()) # Remove duplicates\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff672c1c-ecd7-4134-b912-4a25e1a7f82c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     A. Preprocessing Computation\n",
    "text = clean(new_data, 'new_data')\n",
    "print(text)\n",
    "# Expected Output:\n",
    "# 0       one of the other reviewers has mentioned that ...\n",
    "# 1       a wonderful little production the filming tech...\n",
    "# 2       i thought this was a wonderful way to spend ti...\n",
    "# 3       basically theres a family where a little boy j...\n",
    "# 4       petter matteis love in the time of money is a ...\n",
    "#                               ...                        \n",
    "# 4995    an interesting slasher film with multiple susp...\n",
    "# 4996    i watched this series when it first came out i...\n",
    "# 4997    once again jet li brings his charismatic prese...\n",
    "# 4998    i rented this movie after hearing chris gore s...\n",
    "# 4999    this was a big disappointment for me i think t...\n",
    "# Name: new_data, Length: 4996, dtype: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e5431-cb23-479f-b1ee-8800637ff359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     B. Preprocessed Metadata\n",
    "len(new_data), type(new_data)\n",
    "# Expected Output:\n",
    "# (5000, pandas.core.frame.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07829eb8-22f3-44a1-824d-e9dd48adcca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VI. Tokenize Data\n",
    "# represent each word as a numerical value\n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text) # fit on series of text\n",
    "total_words = len(tokenizer.word_index) + 1 # length of word index\n",
    "print(total_words)\n",
    "# Expected Output:\n",
    "# 21125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b84e1-c87b-4f87-93d9-7a3050d35b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VII. Create input_sequences\n",
    "input_sequences = []\n",
    "for line in text: # for each review\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0] # map each unique word to an integer with tokenizer\n",
    "    # Creating n gram for each review\n",
    "    for i in range(1, len(token_list)): \n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence) # input_sequences is a list of sequences from tokenized reviews\n",
    "# padding sequences so each sequence in input_sequences has the same length\n",
    "max_sequence_len = max([len(x) for x in input_sequences]) # identify length of largest sequence\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre') # pad sequences with zeros\n",
    "\n",
    "# Generate predictor and target data\n",
    "x = input_sequences[:,:-1] # the tokenized sequences minus the last token\n",
    "y = input_sequences[:,-1] # the last token for each tokenized sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec418ff8-a284-49c9-b863-3aa4e52c2ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     A. Sequence Metadata\n",
    "print(\"max_sequence_len:\", max_sequence_len) \n",
    "print(\"Shape of x: \", x.shape, \" Type of x: \", type(x))\n",
    "print(\"Shape of y: \", y.shape, \" Type of y: \", type(y))\n",
    "# Expected Output:\n",
    "# max_sequence_len: 198\n",
    "# Shape of x:  (248366, 197)  Type of x:  <class 'numpy.ndarray'>\n",
    "# Shape of y:  (248366,)  Type of y:  <class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce239c62-0285-49d5-b9e1-cc6947b1d6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VIII. One-hot Encoding\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))\n",
    "# converts matrix to a binary class matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7d300-9c16-4083-8747-170b2e0af54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     A. One-hot Metadata\n",
    "x.shape, type(x), y.shape, type(y)\n",
    "# Expected Output:\n",
    "# ((248366, 197), numpy.ndarray, (248366, 21125), numpy.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c8cdb-2dff-4933-a359-64ff37cafc4f",
   "metadata": {},
   "source": [
    "Even though 'x' and 'y' are numpy arrays, 'np.array()' has to be used in order to avoid errors when fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf370b-d38e-48c0-a2a7-cec19d1ad66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IX. Prepare/Resize Data for Model\n",
    "# convert data to numpy arrays to match model dimensions for fitting\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e001ed-ef24-4b5d-8072-0ddf5982bc1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     A. Resized Metadata\n",
    "x.shape, type(x), y.shape, type(y)\n",
    "# Expected Output:\n",
    "# ((248366, 197), numpy.ndarray, (248366, 21125), numpy.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ecfb51-1835-4e0c-987f-9eaaf8a92bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X. Model 1: Train with LSTM (100), Softmax Dense Layer\n",
    "#     We picked LSTM to train review generation because the content of each\n",
    "#     generated review should be cohesive, and the semantics and meaning of\n",
    "#     each word will depend on the ones before it in the current sentence\n",
    "#     and beyond.\n",
    "#     We picked a Softmax Dense Layer because our output is NLP and is based\n",
    "#     on next-word selection, so a probabilistic mapping of all next possible\n",
    "#     words would be necessary. This is acheived with a Softmax function.\n",
    "#     Other methods like Sigmoid or ReLU do not apply well to our model since\n",
    "#     it is fundamentally for binary and multiclass classification, not\n",
    "#     iterative generation.\n",
    "model = tf.keras.models.Sequential([\n",
    "        layers.Embedding(total_words, 100, input_length = max_sequence_len-1),\n",
    "        layers.LSTM(100),\n",
    "        layers.Dense(total_words, activation='softmax'),\n",
    "    ])\n",
    "model.summary()\n",
    "# Expected Output:\n",
    "# Model: \"sequential_4\"\n",
    "# _________________________________________________________________\n",
    "#  Layer (type)                Output Shape              Param #   \n",
    "# =================================================================\n",
    "#  embedding_4 (Embedding)     (None, 197, 100)          2112500   \n",
    "                                                                 \n",
    "#  lstm_4 (LSTM)               (None, 100)               80400     \n",
    "                                                                 \n",
    "#  dense_4 (Dense)             (None, 21125)             2133625   \n",
    "                                                                 \n",
    "# =================================================================\n",
    "# Total params: 4326525 (16.50 MB)\n",
    "# Trainable params: 4326525 (16.50 MB)\n",
    "# Non-trainable params: 0 (0.00 Byte)\n",
    "# _________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82768a8-2060-4f8a-b31b-e72dc968e72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XI. Model 1 Fitting\n",
    "callback = EarlyStopping(patience=10, monitor= 'loss') # stop training when accuracy doesn't improve\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=20, verbose=1, callbacks=[callback])\n",
    "# Expected Output: [To Be Completed]\n",
    "# Epoch 1/20\n",
    "# 7762/7762 [==============================] - 780s 100ms/step - loss: 6.7600 - accuracy: 0.0910\n",
    "# Epoch 2/20\n",
    "# 7762/7762 [==============================] - 784s 101ms/step - loss: 6.0031 - accuracy: 0.1341\n",
    "# Epoch 3/20\n",
    "# 7762/7762 [==============================] - 785s 101ms/step - loss: 5.6365 - accuracy: 0.1538\n",
    "# Epoch 4/20\n",
    "# 7762/7762 [==============================] - 800s 103ms/step - loss: 5.3247 - accuracy: 0.1701\n",
    "# Epoch 5/20\n",
    "# 7762/7762 [==============================] - 785s 101ms/step - loss: 5.0337 - accuracy: 0.1877\n",
    "# Epoch 6/20\n",
    "# 7762/7762 [==============================] - 797s 103ms/step - loss: 4.7652 - accuracy: 0.2057\n",
    "# Epoch 7/20\n",
    "# 6407/7762 [=======================>......] - ETA: 2:18 - loss: 4.4733 - accuracy: 0.2295"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
